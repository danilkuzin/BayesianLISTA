\documentclass{article} 
\usepackage{nips_2018}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{xcolor}

\usepackage{times}
\usepackage{float}

\graphicspath{{./graphics/}}
\usepackage{subfig}

\allowdisplaybreaks

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}

\title{Supplementary materials for the paper ``Uncertainty propagation in neural networks for sparse coding''}

%\author{ }

\maketitle

\section{Proofs}

\begin{lemma}[Propagation of a Gaussian distribution through soft thresholding]
\label{thm:soft_thresholding}
The distribution of $x^*$ can be parametrised by the probability of being zero, $\omega^*$, the mean $m^*$ and the variance $v^*$ of the truncated Gaussian distribution.
\end{lemma}
\begin{proof}

The probability $\omega^*$ of zero equals to the probability mass of the original distribution from the interval $[-\lambda, \lambda]$
\begin{equation}
\omega^* =\operatorname{P}(x^*=0) = \operatorname{P}(x \in [-\lambda, \lambda]) = \Phi\left(\frac{\lambda-m}{\sqrt{v}}\right) - \Phi\left(\frac{-\lambda-m}{\sqrt{v}}\right).
\end{equation}
where $\Phi(\cdot)$ is the standard Gaussian cumulative distribution function.

The soft thresholding function shifts elements that are greater than $\lambda$  or less than $-\lambda$ towards~$0$. Let $\psi(\cdot)$ denote the density of the soft-thresholded distribution on $x^* \neq 0$, $\phi(\cdot)$ denote the density of the original Gaussian distribution on $x$. Then the first moment of $x^* \neq 0$ is

\begin{equation}
\label{eq:thr_first_moment}
m^* = \int_{-\infty}^{+\infty}t\psi(t)\mathrm{d}t= \int_{-\infty}^{0}t\phi(t-\lambda)\mathrm{d}t + \int_{0}^{+\infty}t\phi(t+\lambda)\mathrm{d}t,
\end{equation}
where
\begin{align}
&\int_{-\infty}^{0}t\phi(t-\lambda)\mathrm{d}t = -\frac{\sqrt{v}}{\sqrt{2\pi}} \exp\left\{\frac{-(\lambda+m)^2}{2v}\right\} + (\lambda+m)\Phi\left(-\frac{\lambda+m}{\sqrt{v}}\right)\\
&\int_{0}^{+\infty}t\phi(t+\lambda)\mathrm{d}t = \frac{\sqrt{v}}{\sqrt{2\pi}} \exp\left\{\frac{-(m - \lambda)^2}{2v}\right\} + (m - \lambda)\left(1 - \Phi\left(-\frac{\lambda-m}{\sqrt{v}}\right)\right)
\end{align}

The second moment of $x^* \neq 0$ is given as
\begin{equation}
\label{eq:thr_second_moment}
s = \int_{-\infty}^{+\infty}t^2\psi(t)\mathrm{d}t = \int_{-\infty}^{0}t^2\phi(t-\lambda)\mathrm{d}t + \int_{0}^{+\infty}t^2\phi(t+\lambda)\mathrm{d}t,
\end{equation}
where
\begin{align}
\begin{split}
&\int_{-\infty}^{0}t^2\phi(t-\lambda)\mathrm{d}t = \\
&-\frac{\sqrt{v}}{\sqrt{2\pi}} (\lambda+m)\exp\left\{\frac{-(\lambda+m)^2}{2v}\right\} + (\sigma^2 + (\lambda+m)^2)\Phi\left(-\frac{\lambda+m}{\sqrt{v}}\right)
\end{split}\\
\begin{split}
&\int_{0}^{+\infty}t^2\phi(t+\lambda)\mathrm{d}t = \\
&\frac{\sqrt{v}}{\sqrt{2\pi}} (m - \lambda)\exp\left\{\frac{-(m - \lambda)^2}{2v}\right\} + (\sigma^2 + (m - \lambda)^2)\left(1 - \Phi\left(\frac{\lambda -m}{\sqrt{v}}\right)\right)
\end{split}
\end{align}

The resulting variance is then
\begin{equation}
v^* = s- (m^*)^2
\end{equation}
\end{proof}

\begin{lemma}[Moments of a spike and slab distribution]
\label{thm:moments_spsl}
Let a random variable $\xi$ have a spike and slab distribution with the probability of spike $\omega$, the slab mean $m$ and slab variance $v$. Then its moments are
\begin{subequations}
\begin{align}
\mathbb{E}\xi &= (1-\omega)m \\
\operatorname{Var}\xi & = (1-\omega)(v + \omega m^2)
\end{align}
\end{subequations}
\end{lemma}

\begin{proof}
\begin{align*}
\begin{split}
\mathbb{E}\xi &= \int t \big(\omega \delta_0(t) + (1 - \omega)\mathcal{N}(t; m, v)\big)\mathrm{d}t \\
& = \omega \int t \delta_0(t)\mathrm{d}t + (1 - \omega)\int t \mathcal{N}(t; m, v)\mathrm{d}t \\
& = (1-\omega)m \\
\mathbb{E}\xi^2 &= \int t^2 \big(\omega \delta_0(t) + (1 - \omega)\mathcal{N}(t; m, v)\big)\mathrm{d}t \\
& = \omega \int t^2 \delta_0(t)\mathrm{d}t + (1 - \omega)\int t^2 \mathcal{N}(t; m, v)\mathrm{d}t \\
& = (1-\omega)(v + m^2) \\
\operatorname{Var}\xi &= \mathbb{E}\xi^2 - \left(\mathbb{E}\xi\right)^2 = (1-\omega)(v + \omega m^2)
\end{split}
\end{align*}
\end{proof}


\begin{lemma}[Product of a Gaussian matrix and a spike and slab vector]
  \label{thm:matrix_vector}
Let $\mathbf{S} \in \mathbb{R}^{D \times D}$ be a matrix of independent Gaussian-distributed random variables: $s_{d'd''} \sim \mathcal{N}(m^s_{d'd''}, v^s_{d'd''})$, and $\widehat{\boldsymbol\beta }\in \mathbb{R}^D$ be a vector with spike-and-slab distributed variables: $\widehat{\beta}_d \sim \omega_d \delta_0 + (1 - \omega_d)\mathcal{N}(m_d, v_d)$. The components of the matrix $\mathbf{S}$ and the vector $\widehat{\boldsymbol\beta}$ are mutually independent. Let $\mathbf{e} \in \mathbb{R}^{D}$ denote the product $\mathbf{S} \widehat{\boldsymbol\beta}$. Then the marginal mean and variance of elements $e_d$ of the vector $\mathbf{e}$ are
\begin{subequations}
\begin{align}
 \mathbb{E}e_d &= \sum_{d'=1}^D m^s_{dd'}(1-\omega_{d'})m_{d'}, \\
 \begin{split}
 \operatorname{Var}e_d &= \sum_{d'=1}^D [(m^s_{dd'})^2(1-\omega_{d'})^2v_{d'} + (1-\omega_{d'})^2(m_{d'})^2v^s_{dd'} + v^s_{dd'}(1-\omega_{d'})^2v_{d'}].
 \end{split}
 \end{align}
\end{subequations}
 \end{lemma}
 \begin{proof}
\begin{flalign*}
	\mathbb{E}e_d &= \sum_{d'=1}^D \mathbb{E}[s_{dd'}\widehat{\beta}_{d'}]  = \sum_{d'=1}^D m^s_{dd'}\mathbb{E}\widehat{\beta}_{d'}\\
	\operatorname{Var}e_d &= \sum_{d'=1}^D \operatorname{Var}[s_{dd'}\widehat{\beta}_{d'}] \\
	&= \sum_{d'=1}^D [(\mathbb{E}s_{dd'})^2 \operatorname{Var}\widehat{\beta}_{d'}
	+ (\mathbb{E}\widehat{\beta}_{d'})^2 \operatorname{Var}s_{dd'} + \operatorname{Var}\widehat{\beta}_{d'} \operatorname{Var}s_{dd'}]
\end{flalign*}
where $\mathbb{E}\widehat{\beta}_{d'}$ and $\operatorname{Var}\widehat{\beta}_{d'}$ are computed according to Lemma~\ref{thm:moments_spsl}.
 \end{proof}
 
\section{Approximation quality}
\label{sec:approx_quality}

\begin{figure}[h]
\centering
\subfloat[Approximation of the \newline product of Gaussians]{\includegraphics[width=0.4\columnwidth]{d_testing}\label{fig:d_testing}}
%\label{fig:d_testing}
\subfloat[Approximation of the propagation through soft thresholding]{\includegraphics[width=0.4\columnwidth]{z_new_testing}\label{fig:z_new_testing}}
\caption{Approximation quality in Bayesian \textsc{lista}}
%\label{fig:z_new_testing}
\end{figure}

We have used two approximations in forward propagation of uncertainty (section 4.3 of the main paper). First, at step 3 during the computation of $\mathbf{e}_l$, a Gaussian matrix is multiplied by a spike and slab vector and their product is approximated with the Gaussian distribution. Second, at steps 2 and 5 the result of soft thresholding of a Gaussian vector is approximated with the spike and slab distribution. In this section we demonstrate that these approximations are close to the real distributions.

Figure \ref{fig:d_testing} demonstrates the comparison of the sampled distribution and approximated distribution for the first approximation at step 3. For the sampled distribution, $10000$ values are sampled from the Gaussian matrix and the spike and slab vector and their product is computed, then one of the dimensionalities is plotted. The parameters of the approximated distribution are computed according to Lemma \ref{thm:matrix_vector}.

Figure \ref{fig:z_new_testing} demonstrates the comparison of the sampled distribution and approximated distribution for the second approximation at steps 2 and 5. For the sampled distribution, $10000$ values are sampled from the Gaussian vector and propagated through soft thresholding, then one of the dimensionalities is plotted. The parameters of the approximated distribution are computed according to Lemma \ref{thm:soft_thresholding}.

\section{Derivation for the normalisation constant}
When the likelihood term for a single data point is incorporated into the approximating distribution~$q$, the normalisation constant of $q$ is expressed as
\begin{equation}
\label{eq:Z}
Z = \int \prod_{d=1}^{D} \mathcal{N}(\beta_d ; [f(\mathbf{y} ; \mathbf{S}, \mathbf{W}, \lambda)]_d, \gamma^{-1}) q(\mathbf{W}, \mathbf{S}, \gamma, \eta) \mathrm{d}\mathbf{W} \mathrm{d}\mathbf{S} \mathrm{d}\gamma \mathrm{d}\eta
\end{equation}
We sample weights $\mathbf{W}$ and $\mathbf{S}$ from $q$ and get $\widehat{\boldsymbol\beta} = f(\mathbf{y} ; \mathbf{S}, \mathbf{W}, \lambda)$ that is the output from the network. According to the proposed uncertainty propagation scheme $\widehat{\boldsymbol\beta}$ is approximated with the spike and slab distribution with the parameters $\boldsymbol\omega^{\widehat{\boldsymbol\beta}}$, $\mathbf{m}^{\widehat{\boldsymbol\beta}}$, and $\mathbf{v}^{\widehat{\boldsymbol\beta}}$. Then we approximate the normalisation constant
\begin{align}
Z &\approx \int \text{Gam} \left(\gamma; \alpha^\gamma, \beta^\gamma\right) \prod_{d=1}^{D} \left[ \mathcal{N}\left(\beta_d ; [\widehat{\boldsymbol\beta}]_d, \gamma^{-1}\right) \right.\\
&\quad \left. \times  \left(\omega^{\widehat{\boldsymbol\beta}}_d \delta_0\left([\widehat{\boldsymbol\beta}]_d\right) + \left(1 - \omega^{\widehat{\boldsymbol\beta}}_d\right)\mathcal{N}\left([\widehat{\boldsymbol\beta}]_d ; m^{\widehat{\boldsymbol\beta}}_d, v^{\widehat{\boldsymbol\beta}}_d\right)\right) \right] \mathrm{d}\widehat{\boldsymbol\beta} \mathrm{d}\gamma  \nonumber\\
& = \prod_{d=1}^{D} \left[\omega^{\widehat{\boldsymbol\beta}}_d \int \mathcal{N}\left(\beta_d ; 0, \gamma^{-1}\right)  \text{Gam} \left(\gamma; \alpha^\gamma, \beta^\gamma\right) d\gamma  \nonumber \right.\\
&\quad + \left(1 - \omega^{\widehat{\boldsymbol\beta}}_d\right)\int \mathcal{T}\left(\beta_d ; [\widehat{\boldsymbol\beta}]_d, \beta^\gamma / \alpha^\gamma, 2\alpha^\gamma\right) 
\left. \vphantom{\int}  \mathcal{N}\left([\widehat{\boldsymbol\beta}]_d ; m^{\widehat{\boldsymbol\beta}}_d, v^{\widehat{\boldsymbol\beta}}_d\right) \mathrm{d}[\widehat{\boldsymbol\beta}]_d\right]  \nonumber\\
\label{eq:Z_approx}
& \approx \prod_{d=1}^D \left[\omega^{\widehat{\boldsymbol\beta}}_d  \mathcal{T}\left(\beta_d ; 0, \beta^\gamma / \alpha^\gamma, 2\alpha^\gamma\right) + \vphantom{m^{\widehat{\boldsymbol\beta}}_d} \left(1 - \omega^{\widehat{\boldsymbol\beta}}_d\right)\mathcal{N}\left(\beta_d ; m^{\widehat{\boldsymbol\beta}}_d,  \beta^\gamma / (\alpha^\gamma - 1) + v^{\widehat{\boldsymbol\beta}}_d\right)\right],
\end{align}
where we have approximated the Student's t density $\mathcal{T}\left(\beta_d ; \widehat{\beta}_d, \beta^\gamma / \alpha^\gamma, 2\alpha^\gamma\right)$ with a Gaussian density with the same mean and variance. The Student's t density can be parametrised in different ways. In this work the following parametrisation is used
\begin{equation}
\mathcal{T}(x; \mu, \beta, \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\pi \nu \beta}} \left(1 + \frac{(x - \mu)^2}{\nu\beta}\right)^{-\frac{\nu + 1}{2}}
\end{equation}
where $\Gamma(\cdot)$ denotes the Gamma function.

\section{Hyperparameter optimisation}
The only hyperparameter in the proposed Bayesian \textsc{lista} model is the shrinkage parameter $\lambda$. It can be optimised using the Type II maximum likelihood procedure. The Type II likelihood, i.e. the evidence $p(\mathbf{B} | \mathbf{Y}, \lambda)$, of the Bayesian \textsc{lista} is equal to the normalisation constant $Z$ (\ref{eq:Z}) computed for the whole training dataset $\{\mathbf{Y}, \mathbf{B}\}$. Given the approximation~(\ref{eq:Z_approx}) the optimal hyperparameter $\lambda$ can be found by a gradient-based optimiser.

In our experiments we use $\lambda = 0.1$, which is determined by grid optimisation (Figure~\ref{fig:lambda_opt}).

\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[width=0.4\columnwidth]{./synthetic_lambda/nmse_validation}}~
\subfloat[]{\includegraphics[width=0.4\columnwidth]{./synthetic_lambda/f_measure_validation}}~
\caption{Grid optimisation for the shrinkage parameter $\lambda$ on the synthetic data.}
\label{fig:lambda_opt}
\end{figure}

\section{Performance measures}
\textsc{nmse} for a batch of data $\{\boldsymbol\beta^{(n)}\}_{n=1}^{N}$ and estimates $\{\widehat{\boldsymbol\beta}^{(n)}\}_{n=1}^{N}$ is computed as
\begin{equation}
\text{\textsc{nmse}} = \frac{1}{N}\sum\limits_{n=1}^N\sqrt{\frac{\sum\limits_{d=1}^D\left([\widehat{\boldsymbol\beta}^{(n)}]_{d} - \beta_d^{(n)}\right)^2}{\sum\limits_{d=1}^D\left(\beta_{d}^{(n)}\right)^2}}
\end{equation}
In sparse coding it is also important to obtain the correct locations of spikes (i.e. zeros) and slabs (i.e. non-zeros) in the estimates. The problem is therefore viewed as a skewed two-class classification problem where the number of spikes is higher than the number of slabs. F measure is used to evaluate the accuracy of such problems. It is defined as the harmonic mean of precision and recall
\begin{equation}
\text{F measure} = 2\dfrac{\text{precision}\cdot\text{recall}}{\text{precision} + \text{recall}},
\end{equation}
where precision is the fraction of estimated slab locations that are correct, recall is the fraction of true slab locations among all predicted slab locations.

\section{Additional results on \textsc{mnist} data}
In contrast to \textsc{lista} and baselines, Bayesian \textsc{lista} provides a posterior estimate for a test data point. Figure~\ref{fig:posterior_samples} shows samples from the posterior for the test image of the digit $7$ and Figure \ref{fig:posterior_distribution} shows the parameters of this posterior.

\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[width=0.33\columnwidth]{posterior_sample_0}}~
\subfloat[]{\includegraphics[width=0.33\columnwidth]{posterior_sample_1}}~
\subfloat[]{\includegraphics[width=0.33\columnwidth]{posterior_sample_2}}
\caption{Samples from the posterior for an image of the digit 7.}
\label{fig:posterior_samples}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[$\boldsymbol\beta$ posterior mean]{\includegraphics[width=0.33\columnwidth]{posterior_mean}}~
\subfloat[$\boldsymbol\beta$ posterior std]{\includegraphics[width=0.33\columnwidth]{posterior_std}}~
\subfloat[$\boldsymbol\beta$ posterior spike indicator]{\includegraphics[width=0.33\columnwidth]{posterior_spike_indicator}}
\caption{Posterior parameters for an image of the digit 7.}
\label{fig:posterior_distribution}
\end{figure}

Here we also provide the comparison of the Bayesian and non-Bayesian \textsc{lista} networks in terms of the clock time for the experiment with $K = 250$ on the \textsc{mnist} data (Figure~\ref{fig:mnist_time}).

\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[width=0.4\columnwidth]{./mnist/250_nmse_valid_time}}~
\subfloat[]{\includegraphics[width=0.4\columnwidth]{./mnist/250_f_measure_valid_time}}~
\caption{Predictive performance over clock time on the \textsc{mnist} data with the observation size $K = 250$.}
\label{fig:mnist_time}
\end{figure}


%\bibliography{bibliography}
%\bibliographystyle{icml2018}

\end{document}
