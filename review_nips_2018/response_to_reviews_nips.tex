\documentclass{article}
\usepackage{nips_2018_author_response}
\usepackage{color}
\usepackage{mathtools}
\begin{document}

We would like to thank you the reviewers for their time and valuable suggestions.

\textbf{Reviewer 1}

\textbf{Q:} ``What is the motivation for being Bayesian about the LISTA algorithm. How does in the uncertainty in the Bayesian LISTA procedure relate to the original linear model.'' 
\textbf{A:} The LISTA algorithm introduces the weights in the model. Their point estimates are being inferred during the training phase, this leads to the uncertainty of the estimation. In our Bayesian model we infer the posterior distributions over these weights to estimate this uncertainty.

\textbf{Q:} ``how does the sparse promoting prior interact with the sparsity promoting non-linearity.'' 
\textbf{A:} In the LISTA model the sequential estimates of beta are obtained at each layer using the previous estimate. At every layer the estimate becomes more sparse due to soft-thresholding non-linearity. We do not specify sparsity priors over the $\beta_l$ - we show that the distribution of these estimates can be approximated with the spike-and-slab distribution, i.e. its sparsity naturally arises from the Gaussian prior on NN weights and sparsity promoting soft thresholding nonlinearity.

\textbf{Q:} ``The approximations in section 4 are part of the approximate inference.''  
\textbf{A:} Yes, they are part of the inference procedure, the model remains the same as in section 3. These approximations are similar to mean-field approximations in variational inference.

\textbf{Q:} ``The assumption of mutual independence over the elements of $S$ and $\beta_{l-1}$'' 
\textbf{A:} Generally speaking, correlation is partially removed due to the soft-thresholding on the previous layer. It requires further research to establish the influence of remaining correlation on the quality of approximation.

\textbf{Reviewer 2}

\textbf{Q:} ``the contribution is quite incremental, with most of the method relying on existing tools for probabilistic back-propagation'' 
\textbf{A:} The main contribution of the paper is in the forward pass, i.e. modelling distributions of the weights and how they propagate through the layers of a NN. We have shown that normally distributed weights propagated though the soft-thresholding non-linearity can be approximated with the spike and slab distribution. Although our inference is based on ideas from probabilistic back-propagation, we expanded this framework. In the original work, PBP was developed only for ReLU non-linearity, we developed it for soft-thresholding non-linearity. Also in contrast to the original work we deal with the multidimensional output of a NN and our NN is recurrent. 

\textbf{Q:} ``Bayesian LISTA outperforms LISTA on NMSE (esp. with more layers/iterations), while LISTA yields better F- measures.''  
\textbf{A:} LISTA gets better F-measure only for number of measurements $K = \{10, 20\}$ for the synthetic data. In all the other experiments Bayesian LISTA shows either competitive results with LISTA or outperforms it.

\textbf{Q:} ``the MNIST experiment presented in the paper is about generating random Gaussian linear combinations of the MNIST images, and then trying to recover the original coefficients. No sparsity is imposed.'' 
\textbf{A:} Sparsity in the MNIST experiment is in images themselves. Flatten images represent vectors with a lot of zeros corresponding to background pixels.

\textbf{Reviewer 3}

\textbf{Q:} `` The update rules for the backpropagation algorithm should be given for each parameter, at least in the supplementary.'' 
\textbf{A:} We included details of the backpropagation algorithm that are distinguish and original for this paper. The general idea and formula of updating parameters based on derivatives of the normalisation constant is not new, therefore, we provided only derivation for the normalisation constant, which is different. We will add full details in the supplementary to make the paper self-contained. Thank you for your suggestion. 

\textbf{Q:} ``My main concern is that the loss used to train the network is not detailed.'' 
\textbf{A:} Proposed Bayesian LISTA does not directly minimise any loss while training the network, we rather minimise the KL-divergence between approximating and posterior distribution on parameters. Backpropagation is used for the ADF updates of the parameters for computing derivatives of the normalisation constant. LISTA minimises the squared error between the predicted output and the given output for training network. Eq. (1) provides the general formulation of the sparse coding problem. It is not used for training networks. The parameter $\lambda$ was introduced in ISTA. The value $0.1$ was shown as this was found optimal for all algorithms on the synthetic data (plots are given in the supplementary).

\textbf{Q:} ``Only 200 samples are used from the 60,000 available, suggesting that the proposed technique scale poorly. The results are also displayed relatively to the number of training iteration and not with varying number L of layers/iterations as in the previous experiment'' 
\textbf{A:} A limited number of samples are used to show benefits of using a Bayesian method such as Bayesian LISTA over a frequentist method such as LISTA. As shown in the example of increasing the number of measurements when the data is not scarce, Bayesian LISTA and LISTA provides competitive results. The similar behaviour is demonstrated when we increase the size of training data. We will add the experiments with varying L as suggested. Thank you. 

\end{document}
