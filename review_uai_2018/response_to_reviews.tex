\documentclass{article}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{color}
\usepackage{mathtools}
\begin{document}

\title{UAI 2018 Reply to Reviewers}
\date{}
\maketitle

\section*{Reviewer 1}
Thank you for the helpful suggestions.

Q: It's not so surprising that probabilistic versions of ISTA should do better in the limited dictionary case. What I'm curious about is how fast the proposed algorithm is, not in terms of iterations, but wall clock time.

A: We will add the wall clock time comparison. Preliminary results show that LISTA and Bayesian LISTA require the same running time to achieve similar accuracy. One iteration of Bayesian LISTA takes more time, but the convergence rate is also higher for Bayesian LISTA.
\newline
\newline

Q: Please enlarge the figures, as they are very hard to read.

A: We will enlarge the figures and move some of the proofs from section 4 to appendix to get space for figures.
\newline
\newline

Q: What's the intuitive reason for baseline ISTA/FISTA to have better performance in the synthetic data case?

A: We believe this is because there is not enough data for LISTA and Bayesian LISTA to learn the matrices.
\newline
\newline

Q: What is the expected difference between Bayesian and non-Bayesian as the number of layers increases (ie, you expect the gap to close, stay the same, etc.)?

A: Based on the current results, we expect the gap to stay the same, we will add experiments with more layers.
\newline
\newline

Q: Do you expect it to be possible to replace the soft thresholding with some other nonlinearity and to produce a similar algorithms? This isn't something I expect to be done easily, but do you expect it to be feasible, or is the combination of soft threshold with the spike/slab distribution critical?

A: In sparse coding the soft-thresholding nonlinearity is the most popular as it is used in the optimal solution of the lasso method [1]. There are several other nonlinearities that are used in sparse coding, for example some of them are described in sec. IV-B of [2]. We can approximate other nonlinearities with the spike and slab distribution to see if these approximations are different in quality. This is an interesting future direction.

The main motivation of using the spike and slab distribution is to obtain approximations of the sparse unknown vector $\beta$, that is critical for sparse coding. There is no intuition or motivation to use these distributions for networks with dense output.

[1] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

[2] Borgerding, M., Schniter, P., Rangan, S. (2017). AMP-Inspired Deep Networks for Sparse Linear Inverse Problems. IEEE Transactions on Signal Processing.

\section*{Reviewer 2}
Thank you for the helpful suggestions.

Q: It's not clear that a machine learning venue is appropriate. ... I found the writing unclear and confusing, as the authors switch back and forth between a neural network interpretation of LISTA, and the standard sparse coding interpretation. I imagine both interpretations would be standard for e.g. a signal processing audience?

A: We thank the reviewer for raising this important question. Indeed, the field of sparse coding is on the border between signal processing and machine learning. Recently different works on deep sparse coding have been of interest in the machine learning community [1] -- [5].

We believe since our method allows to estimate the uncertainty of the deep sparse coding algorithms, it suits the UAI conference.

[1] Wang, Z., Liu, D., Yang, J., Han, W., Huang, T. (2015). Deep networks for image super-resolution with sparse prior. ICCV.

[2] Sprechmann, P., Bronstein, A. M., Sapiro, G. (2015). Learning efficient sparse and low rank models. TPAMI.

[3] Moreau, T., Bruna, J. (2017). Understanding Trainable Sparse Coding via Matrix Factorization. ICLR.

[4] Papyan, V., Romano, Y., Elad, M. (2017). Convolutional neural networks analyzed via convolutional sparse coding. JMLR.

[5] He, H., Xin, B., Ikehata, S., Wipf, D. (2017). From Bayesian Sparsity to Gated Recurrent Nets. NIPS.
\newline
\newline

Q: I am unclear on how one can compute NMSE/F-measure for MNIST in figure 5/8, unless the ground truth sparsity is known i.e. if this is not real data. Is the experiment composed of real data and synthetic weights? While the focus is on estimation, for real data, would be useful to see prediction results (again, I am assuming ground truth is not known).

A: For MNIST we simulate sparse measurements $y$, therefore we know the ground truth for sparse $\beta$. Sections 6.1 and 6.2 are devoted to experiments on predictive performance.
\newline
\newline

Q: There are some unnecessary propositions e.g. Proposition 1/4, which refer to textbook results. Perhaps a simple citation is enough to save some space?

A: We will move proofs to the appendix.

\section*{Reviewer 3}
Thank you for the helpful suggestions.

Q: What qualifies equation (11) to be a likelihood distribution?

A: We assume that $\beta$, which are available for the training dataset, are outputs of the neural net corrupted by the additive Gaussian-distributed noise. Therefore, (11) is the Gaussian likelihood.
\newline
\newline

Q: The uncertainty propagation in section 4 refers to a distribution over $\beta$. Which distribution are the authors referring to? Conditioned upon $Y,W,S,\gamma,\lambda$ as in (13)? Related: perhaps the method is better explained as a variational approximation on $p(B,W,S,\gamma,\lambda|Y)$?

A: Section 4 refers to distribution over $\widehat{\beta}$, which is the output of each layer of the neural network and ultimately the output of the whole network, which is denoted as $f(y; W, S, \lambda)$ in eq. (11). This $\widehat{\beta}$ is an approximation of the true $\beta$.

Yes, the distribution over $\widehat{\beta}$ is conditioned on $Y, W, S, \lambda$, but it is not conditioned on $\gamma$ as this is the precision of the noise that relates $\beta$, which we have during training as a target, and $\widehat{\beta}$, which is the output of the neural network.

The proposed inference is a variational approximation of the true posterior in the context of EP and ADF frameworks.
\newline
\newline

Q: The exact meaning of “Then we generate observations as $y = X \beta$, where $\beta$ are images converted to vectors.”(Section 6.2) is not quite clear. Are the $\beta$ now flattened images, or the same sparse representations? Which dimensionality do they have?

A: Yes, each beta is a flattened image of size $(784 \times 1)$
\newline
\newline

Q: Why do the authors restrict themselves to so few training and validation images (100 each)?

A: Bayesian models in general show themselves especially useful dealing with limited data, when the frequentist counterparts may suffer from inadequate maximum likelihood estimates. Therefore, we were particularly interested in study on limited data, where the frequentist LISTA might have lower accuracy, to better show how Bayesian LISTA may provide benefits over the frequentist LISTA. We expect the frequentist LISTA to have very good results when there is a lot of data, which has been shown in the original paper and extensions.

We appreciate that it would be useful to have also confirmation that on larger datasets the frequentist and Bayesian LISTA show competitive results. We will add the experiments on larger datasets.
\newline
\newline

Q: Why is $\lambda$ fixed to 0.1?

A: It was empirically set based on grid search. This value looks reasonable, since we use normalised data. As we mention in the paper the type II maximum likelihood can be used to find this value.
\newline
\newline

Q: In the Discussion, the use with overcomplete dictionaries is mentioned as possible but no reasoning is given as to why undercomplete dictionaries were used and why they deviated from the method of the original paper.

A: Both overcomplete and undercomplete cases represent very important problems that conveniently can be solved with the same methods. In our previous work we dealt with undercomplete dictionaries and therefore we had the experiments with undercomplete dictionaries. Due to the space limit we could not include experiments for both cases.
\newline
\newline

Q: The paper contains a number of grammatical errors and typos

A: Thank you for pointing this. We will check and correct them.
\newline
\newline

Q: Furthermore, the Figures are very small to a point where they are illegible on a DIN A4 printout.
If some of the proofs and calculations were moved to an Appendix, there would be room for readable Figures and more explanations in Sections 3 and 6.

A: We will move some proofs to the appendix and enlarge the figures.


\end{document}
